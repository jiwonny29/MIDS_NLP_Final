{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing solution based on a discriminative neural network (BiLSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and apply the model to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 06:49:11.301612: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-11 06:49:11.334202: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-11 06:49:11.334236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-11 06:49:11.335477: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-11 06:49:11.341398: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-11 06:49:11.342341: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-11 06:49:12.318177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               49664     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                2064      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 371779 (1.42 MB)\n",
      "Trainable params: 371779 (1.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2303/2303 [==============================] - 320s 137ms/step - loss: 0.8085 - accuracy: 0.6193 - val_loss: 0.7052 - val_accuracy: 0.6894\n",
      "Epoch 2/100\n",
      "2303/2303 [==============================] - 312s 136ms/step - loss: 0.6777 - accuracy: 0.7097 - val_loss: 0.6953 - val_accuracy: 0.6966\n",
      "Epoch 3/100\n",
      "2303/2303 [==============================] - 313s 136ms/step - loss: 0.6257 - accuracy: 0.7364 - val_loss: 0.6745 - val_accuracy: 0.7062\n",
      "Epoch 4/100\n",
      "2303/2303 [==============================] - 317s 137ms/step - loss: 0.5783 - accuracy: 0.7597 - val_loss: 0.6819 - val_accuracy: 0.7047\n",
      "Epoch 5/100\n",
      "2303/2303 [==============================] - 311s 135ms/step - loss: 0.5415 - accuracy: 0.7771 - val_loss: 0.7094 - val_accuracy: 0.7090\n",
      "Epoch 6/100\n",
      "2303/2303 [==============================] - 311s 135ms/step - loss: 0.4989 - accuracy: 0.7964 - val_loss: 0.7180 - val_accuracy: 0.7019\n",
      "Epoch 7/100\n",
      "2303/2303 [==============================] - 311s 135ms/step - loss: 0.4651 - accuracy: 0.8117 - val_loss: 0.7363 - val_accuracy: 0.7002\n",
      "Epoch 8/100\n",
      "2303/2303 [==============================] - 310s 135ms/step - loss: 0.4317 - accuracy: 0.8271 - val_loss: 0.7872 - val_accuracy: 0.6988\n",
      "Epoch 9/100\n",
      "2303/2303 [==============================] - 308s 134ms/step - loss: 0.4034 - accuracy: 0.8399 - val_loss: 0.8561 - val_accuracy: 0.6942\n",
      "Epoch 10/100\n",
      "2303/2303 [==============================] - 302s 131ms/step - loss: 0.3868 - accuracy: 0.8462 - val_loss: 0.9032 - val_accuracy: 0.6940\n",
      "Epoch 11/100\n",
      "2303/2303 [==============================] - 299s 130ms/step - loss: 0.3596 - accuracy: 0.8588 - val_loss: 0.9287 - val_accuracy: 0.6970\n",
      "Epoch 12/100\n",
      "2303/2303 [==============================] - 301s 131ms/step - loss: 0.3344 - accuracy: 0.8673 - val_loss: 1.0031 - val_accuracy: 0.6870\n",
      "Epoch 13/100\n",
      "2303/2303 [==============================] - 300s 130ms/step - loss: 0.3146 - accuracy: 0.8767 - val_loss: 1.0454 - val_accuracy: 0.6872\n",
      "Epoch 14/100\n",
      "2303/2303 [==============================] - 308s 134ms/step - loss: 0.2978 - accuracy: 0.8842 - val_loss: 1.1235 - val_accuracy: 0.6883\n",
      "Epoch 15/100\n",
      "2303/2303 [==============================] - 306s 133ms/step - loss: 0.2804 - accuracy: 0.8910 - val_loss: 1.1665 - val_accuracy: 0.6779\n",
      "Epoch 16/100\n",
      "2303/2303 [==============================] - 309s 134ms/step - loss: 0.2666 - accuracy: 0.8972 - val_loss: 1.2289 - val_accuracy: 0.6847\n",
      "Epoch 17/100\n",
      "2303/2303 [==============================] - 300s 130ms/step - loss: 0.2506 - accuracy: 0.9035 - val_loss: 1.3017 - val_accuracy: 0.6844\n",
      "Epoch 18/100\n",
      "2303/2303 [==============================] - 301s 131ms/step - loss: 0.2398 - accuracy: 0.9086 - val_loss: 1.3471 - val_accuracy: 0.6746\n",
      "Epoch 19/100\n",
      "2303/2303 [==============================] - 300s 130ms/step - loss: 0.2310 - accuracy: 0.9110 - val_loss: 1.3737 - val_accuracy: 0.6740\n",
      "Epoch 20/100\n",
      "2303/2303 [==============================] - 298s 130ms/step - loss: 0.2174 - accuracy: 0.9174 - val_loss: 1.4760 - val_accuracy: 0.6790\n",
      "576/576 [==============================] - 10s 17ms/step - loss: 0.7094 - accuracy: 0.7090\n",
      "Real Data Evaluation - Loss: 0.7094, Accuracy: 0.7090\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "\n",
    "# Data processing function\n",
    "def data_processing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Load real data\n",
    "df_real3 = pd.read_csv(\"balanced_data.csv\")\n",
    "\n",
    "# Apply data processing to the 'review' column\n",
    "df_real3[\"review\"] = df_real3[\"review\"].apply(data_processing)\n",
    "\n",
    "# Preprocess real data\n",
    "texts_real3 = df_real3['review'].values\n",
    "labels_real3 = df_real3['sentiment'].values\n",
    "\n",
    "# Ensure sentiment labels are in the range [0, 2]\n",
    "labels_real3 = labels_real3 + 1  # Convert -1 to 0, keep 0 and 1 as is\n",
    "\n",
    "# Tokenization for real data\n",
    "max_words3 = 10000\n",
    "max_length_real3 = 100\n",
    "tokenizer_real3 = Tokenizer(num_words=max_words3)\n",
    "tokenizer_real3.fit_on_texts(texts_real3)\n",
    "sequences_real3 = tokenizer_real3.texts_to_sequences(texts_real3)\n",
    "padded_sequences_real3 = pad_sequences(sequences_real3, maxlen=max_length_real3, truncating='post')\n",
    "\n",
    "# Train/test split for real data\n",
    "X_train_real3, X_test_real3, y_train_real3, y_test_real3 = train_test_split(\n",
    "    padded_sequences_real3, labels_real3, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build and compile the model for real data (adjusted architecture)\n",
    "model_real3 = Sequential([\n",
    "    Embedding(input_dim=max_words3, output_dim=32, input_length=max_length_real3),\n",
    "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')  # Adjusted to 3 output categories\n",
    "])\n",
    "\n",
    "adam_real3 = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model_real3.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=adam_real3,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_real3.summary()\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping_real3 = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', patience=15, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_real3.fit(\n",
    "    X_train_real3, y_train_real3, epochs=100,\n",
    "    validation_data=(X_test_real3, y_test_real3),\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping_real3]\n",
    ")\n",
    "\n",
    "# Evaluate the model on real data\n",
    "real_results3 = model_real3.evaluate(X_test_real3, y_test_real3)\n",
    "print(\"Real Data Evaluation - Loss: {:.4f}, Accuracy: {:.4f}\".format(real_results3[0], real_results3[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_real3.save(\"blstm_real.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 10s 17ms/step\n",
      "Real Data Evaluation:\n",
      "Accuracy: 0.7090\n",
      "Precision: 0.7070\n",
      "Recall: 0.7090\n",
      "F1 Score: 0.7078\n",
      "Confusion Matrix:\n",
      "[[4417 1353  378]\n",
      " [1467 3751 1000]\n",
      " [ 314  849 4891]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Evaluate the model on real data\n",
    "real_predictions3 = model_real3.predict(X_test_real3)\n",
    "real_predictions_classes3 = real_predictions3.argmax(axis=-1)\n",
    "\n",
    "# Calculate and print accuracy, precision, recall, and F1-score\n",
    "accuracy_real3 = accuracy_score(y_test_real3, real_predictions_classes3)\n",
    "precision_real3 = precision_score(y_test_real3, real_predictions_classes3, average='weighted')\n",
    "recall_real3 = recall_score(y_test_real3, real_predictions_classes3, average='weighted')\n",
    "f1_real3 = f1_score(y_test_real3, real_predictions_classes3, average='weighted')\n",
    "\n",
    "print(\"Real Data Evaluation:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_real3))\n",
    "print(\"Precision: {:.4f}\".format(precision_real3))\n",
    "print(\"Recall: {:.4f}\".format(recall_real3))\n",
    "print(\"F1 Score: {:.4f}\".format(f1_real3))\n",
    "\n",
    "# Display confusion matrix\n",
    "conf_matrix_real3 = confusion_matrix(y_test_real3, real_predictions_classes3)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_real3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and apply the model to synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 128)               49664     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                2064      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 371779 (1.42 MB)\n",
      "Trainable params: 371779 (1.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      " 143/1250 [==>...........................] - ETA: 2:16 - loss: 0.9269 - accuracy: 0.5852"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 169s 132ms/step - loss: 0.1812 - accuracy: 0.9261 - val_loss: 0.0494 - val_accuracy: 0.9787\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 164s 131ms/step - loss: 0.0549 - accuracy: 0.9749 - val_loss: 0.0468 - val_accuracy: 0.9794\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 165s 132ms/step - loss: 0.0522 - accuracy: 0.9751 - val_loss: 0.0464 - val_accuracy: 0.9794\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 163s 130ms/step - loss: 0.0497 - accuracy: 0.9756 - val_loss: 0.0463 - val_accuracy: 0.9789\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 164s 131ms/step - loss: 0.0586 - accuracy: 0.9753 - val_loss: 0.0464 - val_accuracy: 0.9799\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 164s 131ms/step - loss: 0.0491 - accuracy: 0.9771 - val_loss: 0.0459 - val_accuracy: 0.9797\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 165s 132ms/step - loss: 0.0494 - accuracy: 0.9762 - val_loss: 0.0463 - val_accuracy: 0.9796\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 165s 132ms/step - loss: 0.0489 - accuracy: 0.9769 - val_loss: 0.0460 - val_accuracy: 0.9796\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 166s 133ms/step - loss: 0.0481 - accuracy: 0.9768 - val_loss: 0.0463 - val_accuracy: 0.9796\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 167s 133ms/step - loss: 0.0483 - accuracy: 0.9768 - val_loss: 0.0467 - val_accuracy: 0.9792\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 169s 135ms/step - loss: 0.0480 - accuracy: 0.9769 - val_loss: 0.0489 - val_accuracy: 0.9776\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 169s 135ms/step - loss: 0.0489 - accuracy: 0.9769 - val_loss: 0.0463 - val_accuracy: 0.9800\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 166s 133ms/step - loss: 0.0481 - accuracy: 0.9770 - val_loss: 0.0462 - val_accuracy: 0.9796\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 163s 131ms/step - loss: 0.0478 - accuracy: 0.9769 - val_loss: 0.0460 - val_accuracy: 0.9798\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 159s 127ms/step - loss: 0.0501 - accuracy: 0.9769 - val_loss: 0.0474 - val_accuracy: 0.9798\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 159s 127ms/step - loss: 0.0478 - accuracy: 0.9770 - val_loss: 0.0464 - val_accuracy: 0.9798\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 159s 127ms/step - loss: 0.0477 - accuracy: 0.9774 - val_loss: 0.0466 - val_accuracy: 0.9784\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 160s 128ms/step - loss: 0.0479 - accuracy: 0.9772 - val_loss: 0.0459 - val_accuracy: 0.9795\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 160s 128ms/step - loss: 0.0478 - accuracy: 0.9769 - val_loss: 0.0478 - val_accuracy: 0.9778\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 164s 131ms/step - loss: 0.0478 - accuracy: 0.9768 - val_loss: 0.0458 - val_accuracy: 0.9799\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 163s 130ms/step - loss: 0.0478 - accuracy: 0.9770 - val_loss: 0.0461 - val_accuracy: 0.9787\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 164s 131ms/step - loss: 0.0487 - accuracy: 0.9767 - val_loss: 0.0462 - val_accuracy: 0.9796\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 158s 126ms/step - loss: 0.0476 - accuracy: 0.9768 - val_loss: 0.0458 - val_accuracy: 0.9799\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 156s 125ms/step - loss: 0.0474 - accuracy: 0.9769 - val_loss: 0.0462 - val_accuracy: 0.9779\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 156s 125ms/step - loss: 0.0476 - accuracy: 0.9774 - val_loss: 0.0460 - val_accuracy: 0.9796\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 156s 125ms/step - loss: 0.0472 - accuracy: 0.9767 - val_loss: 0.0456 - val_accuracy: 0.9796\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 155s 124ms/step - loss: 0.0474 - accuracy: 0.9770 - val_loss: 0.0462 - val_accuracy: 0.9791\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.0463 - accuracy: 0.9800\n",
      "Synthetic Data Evaluation - Loss: 0.0463, Accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load synthetic data\n",
    "df_synthetic = pd.read_csv(\"synthetic_data_nltk.csv\")\n",
    "\n",
    "# Convert sentiment labels to numeric values\n",
    "label_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "df_synthetic['label'] = df_synthetic['label'].map(label_mapping)\n",
    "\n",
    "# Tokenization for synthetic data\n",
    "max_words = 10000  # Limit the number of words to consider\n",
    "max_length_synthetic = 100  # Adjusted to a reasonable value\n",
    "tokenizer_synthetic = Tokenizer(num_words=max_words)\n",
    "tokenizer_synthetic.fit_on_texts(df_synthetic['text'].values)\n",
    "sequences_synthetic = tokenizer_synthetic.texts_to_sequences(df_synthetic['text'].values)\n",
    "padded_sequences_synthetic = pad_sequences(sequences_synthetic, maxlen=max_length_synthetic, truncating='post')\n",
    "\n",
    "# Train/test split for synthetic data\n",
    "X_train_synthetic, X_test_synthetic, y_train_synthetic, y_test_synthetic = train_test_split(\n",
    "    padded_sequences_synthetic, df_synthetic['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build and compile the model for synthetic data\n",
    "model_synthetic = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=32, input_length=max_length_synthetic),\n",
    "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')  # Updated to have 3 output units for negative, neutral, and positive\n",
    "])\n",
    "\n",
    "adam_synthetic = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model_synthetic.compile(\n",
    "    loss='sparse_categorical_crossentropy',  # Updated to use sparse categorical crossentropy for multi-class classification\n",
    "    optimizer=adam_synthetic,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_synthetic.summary()\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping_synthetic = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', patience=15, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_synthetic.fit(\n",
    "    X_train_synthetic, y_train_synthetic, epochs=100,\n",
    "    validation_data=(X_test_synthetic, y_test_synthetic),\n",
    "    batch_size=32,  # Adjusted to a smaller batch size\n",
    "    callbacks=[early_stopping_synthetic]\n",
    ")\n",
    "\n",
    "# Evaluate the model on synthetic data\n",
    "synthetic_results = model_synthetic.evaluate(X_test_synthetic, y_test_synthetic)\n",
    "print(\"Synthetic Data Evaluation - Loss: {:.4f}, Accuracy: {:.4f}\".format(synthetic_results[0], synthetic_results[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_synthetic.save(\"blstm_synthetic.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 16ms/step\n",
      "Synthetic Data Evaluation:\n",
      "Accuracy: 0.9800\n",
      "Precision: 0.9801\n",
      "Recall: 0.9800\n",
      "F1 Score: 0.9800\n",
      "Confusion Matrix:\n",
      "[[3286    0   72]\n",
      " [   0 3386    0]\n",
      " [ 128    0 3128]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the model on synthetic data\n",
    "synthetic_predictions = model_synthetic.predict(X_test_synthetic)\n",
    "synthetic_predictions_classes = synthetic_predictions.argmax(axis=-1)\n",
    "\n",
    "# Calculate and print accuracy, precision, recall, and F1-score\n",
    "accuracy_synthetic = synthetic_results[1]\n",
    "precision_synthetic = precision_score(y_test_synthetic, synthetic_predictions_classes, average='weighted')\n",
    "recall_synthetic = recall_score(y_test_synthetic, synthetic_predictions_classes, average='weighted')\n",
    "f1_synthetic = f1_score(y_test_synthetic, synthetic_predictions_classes, average='weighted')\n",
    "\n",
    "print(\"Synthetic Data Evaluation:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_synthetic))\n",
    "print(\"Precision: {:.4f}\".format(precision_synthetic))\n",
    "print(\"Recall: {:.4f}\".format(recall_synthetic))\n",
    "print(\"F1 Score: {:.4f}\".format(f1_synthetic))\n",
    "\n",
    "# Display confusion matrix\n",
    "conf_matrix_synthetic = confusion_matrix(y_test_synthetic, synthetic_predictions_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_synthetic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
